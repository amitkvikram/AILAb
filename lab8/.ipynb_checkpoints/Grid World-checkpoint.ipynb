{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class State:\n",
    "    def __init__(self, i, j):\n",
    "        self.row = i\n",
    "        self.col = j\n",
    "\n",
    "    \n",
    "class Environment:\n",
    "    def __init__(self, n, m):\n",
    "        self.rowNum = n\n",
    "        self.colNum = m\n",
    "        \n",
    "        #reward for R: S -> relaNumber\n",
    "        self.reward = np.array([[0, 0.45, 1, 0.9]\n",
    "                               ,[0.23, 1.25, 0, 0]\n",
    "                               ,[0, 0.45, 0.75, 0]\n",
    "                               ,[0.85, 1.5, 2.5, 0.85]])\n",
    "        \n",
    "        #Initial Policy for policy iteration  \n",
    "        self.policy =  np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "\n",
    "#         self.policy =  np.array([['right', 'down', 'down', 'left']\n",
    "#                      ,['right', 'down', 'down', 'left']\n",
    "#                      ,['right', 'right', 'down', 'left']\n",
    "#                      ,['right', 'up', 'up', 'left']])\n",
    "        \n",
    "        #Probabilty P(action) : A x A-> realNumber\n",
    "                                    #left, right, up, down\n",
    "        self.probability = np.array([[0.8, 0, 0.1, 0.1]    #left\n",
    "                                    ,[0, 0.8, 0.1, 0.1]   #right\n",
    "                                    ,[0.1, 0.1 , 0.8, 0]  #up\n",
    "                                    ,[0.1, 0.1, 0, 0.8]])  #down\n",
    "        \n",
    "        #Initial Value Function\n",
    "        self.value = np.zeros((self.rowNum, self.colNum))\n",
    "        \n",
    "        #Possible Actions\n",
    "        self.action = {\"left\": (0, -1), \"right\": (0, 1), \"up\": (-1, 0), \"down\": (1, 0)}\n",
    "        \n",
    "    #fucntion which takes a state and checks if state if valid i.e state is not outside of grid\n",
    "    def isValidState(self, state):\n",
    "        return (state.row >=0 and state.row < self.rowNum and state.col < self.colNum and state.col >=0)\n",
    "        \n",
    "        \n",
    "    #function which takes current state and action AND returns nextState\n",
    "    def getNextState(self, state, action):\n",
    "        nextState = copy.copy(state)\n",
    "        nextState.row+=self.action[action][0]\n",
    "        nextState.col+=self.action[action][1]\n",
    "        \n",
    "        if(self.isValidState(nextState)):\n",
    "            return nextState\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "class agent:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very Small Delta breaking....\n",
      "Reward\n",
      " [[ 0.    0.45  1.    0.9 ]\n",
      " [ 0.23  1.25  0.    0.  ]\n",
      " [ 0.    0.45  0.75  0.  ]\n",
      " [ 0.85  1.5   2.5   0.85]]\n",
      "Optimal Values\n",
      " [[ 80.08545771  90.03320523  90.06615935  80.86201694]\n",
      " [ 89.77733064  91.85216188  91.10605921  89.09646536]\n",
      " [ 89.703789    91.79705689  92.42491992  90.26303576]\n",
      " [ 82.03060645  91.53078465  93.00403337  83.26023497]]\n",
      "Optimal Policy\n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'left']\n",
      " ['right' 'right' 'down' 'left']\n",
      " ['right' 'up' 'up' 'left']]\n",
      "Plot of infinity norm of V_t - V*\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAElCAYAAAALP/6mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKIiISAkVBRERKaGiICIiJf4/JC8f5nFut9wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexToAction = {0:\"left\", 1:\"right\", 2:\"up\", 3:\"down\"}\n",
    "actionToIndex = {\"left\":0, \"right\":1, \"up\":2, \"down\":3}\n",
    "\n",
    "env = Environment(4, 4)\n",
    "\n",
    "def normInfinity(currValue, optimalValue):\n",
    "    maxDiff = 0\n",
    "    for i in range(env.rowNum):\n",
    "        for j in range(env.colNum):\n",
    "            maxDiff = max(maxDiff, abs(currValue[i, j] - optimalValue[i, j]))\n",
    "    return maxDiff\n",
    "            \n",
    "def valueIteration(gamma):\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -100\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "    #                 print(currState.row, currState.col, currReward)\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "    #     print(delta)\n",
    "        if(delta < theta):\n",
    "            print(\"Very Small Delta breaking....\")\n",
    "            break\n",
    "            \n",
    "    #get Optimal Policy\n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            maxReward = -10\n",
    "            optimalAction = \"left\"\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                        + gamma* env.value[nextState.row, nextState.col]))\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            env.policy[currState.row, currState.col] = optimalAction\n",
    "                \n",
    "\n",
    "    print(\"Reward\\n\", env.reward)\n",
    "    print(\"Optimal Values\\n\",env.value)\n",
    "    print(\"Optimal Policy\\n\", env.policy)\n",
    "    \n",
    "    #Plot ||V_t - V_inf||_inf\n",
    "    optimalValue = env.value.copy()\n",
    "    env.value = np.zeros((env.rowNum, env.colNum))\n",
    "    iter1 = 0\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -1\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "    #                 print(currState.row, currState.col, currReward)\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "    #     print(delta)\n",
    "        if(delta < theta):\n",
    "            break\n",
    "        diff.append(normInfinity(env.value, optimalValue))\n",
    "    print(\"Plot of infinity norm of V_t - V*\")\n",
    "    plt.plot(diff, 'r-', label = \"V-V*\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Infinite norm\")\n",
    "    plt.title(\"|| V_t - V* ||_inf  vs  iteration\\n\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "valueIteration(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPolicy(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    \n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                prevVal = value[row, col]\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, action)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])) \n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "                delta = max(delta, abs(value[row, col] - prevVal))\n",
    "        if(delta < theta):\n",
    "#             print(\"Very Small delta\")\n",
    "            break\n",
    "#     print(value)\n",
    "    \n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            optimalAction = \"left\"\n",
    "            maxReward = -100\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=p*(env.reward[currState.row, currState.col] + \n",
    "                                       gamma*value[nextState.row, nextState.col])\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            \n",
    "            policy[row, col] = optimalAction\n",
    "            \n",
    "    return policy\n",
    "\n",
    "def getValue(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    print(policy)\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])) \n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "#         print(\"value = \", value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy\n",
      " [['down' 'down' 'right' 'left']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' 'up' 'up' 'left']]\n",
      "[['down' 'down' 'right' 'left']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' 'up' 'up' 'left']]\n",
      "Optimal Value\n",
      " [[  94.9639887    96.9886809    95.02512563   94.97487437]\n",
      " [  95.9232209    97.5138191   160.43969849  158.93331151]\n",
      " [  96.6598191    97.2361809   162.06030151  160.53869849]\n",
      " [  97.6361809    97.7638191   162.93969849  162.16030151]]\n",
      "[['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'left']\n",
      " ['right' 'right' 'down' 'left']\n",
      " ['right' 'up' 'up' 'left']]\n",
      "Optimal Value1\n",
      " [[ 157.78173856  159.37549349  159.83530151  159.13694849]\n",
      " [ 159.15549349  160.53080151  160.43969849  158.83530151]\n",
      " [ 159.28080151  160.88969849  162.06030151  160.43969849]\n",
      " [ 160.02299349  160.78080151  162.93969849  162.16030151]]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "env = Environment(4, 4)\n",
    "\n",
    "def policyIteration():\n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for iter1 in tqdm(range(10)):\n",
    "\n",
    "        prevPolicy = policy.copy()\n",
    "        policy = getPolicy(env, policy, gamma)\n",
    "#         print(policy)\n",
    "#         print(getValue(env, policy, gamma))\n",
    "        if(np.array_equal(policy, prevPolicy)):\n",
    "            print(\"Found optimal Policy\")\n",
    "            break\n",
    "        \n",
    "    print(\"Optimal Policy\\n\", policy)\n",
    "    print(\"Optimal Value\\n\", getValue(env, policy, gamma))\n",
    "    \n",
    "policyIteration()\n",
    "\n",
    "policy =  np.array([['right', 'down', 'down', 'left']\n",
    " ,['right', 'down', 'down', 'left']\n",
    " ,['right', 'right', 'down', 'left']\n",
    " ,['right', 'up', 'up', 'left']])\n",
    "\n",
    "print(\"Optimal Value1\\n\", getValue(env, policy, gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.461"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.99*(160*0.8) + 0.99*(159*0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
