{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 08\n",
    "#### Problem: Grid World\n",
    "##### Members:\n",
    "        1. Amit Vikram Singh(111601001)\n",
    "        2. Kuldeep Singh Bhandari(111601009)\n",
    "Logic :\n",
    "\n",
    "> 1. **Value Iteration** : In value iteration, first we find optimal value for the given grid world and then find the optimal policy. \n",
    "2. **Policy Iteration** : In policy iteration, we have an initial pseudo policy and using that, we will find value corresponding to that policy and then we will update the policy in greedy fashion. We keep on doing that until current policy becomes equal to the previous policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class State:\n",
    "    def __init__(self, i, j):\n",
    "        self.row = i\n",
    "        self.col = j\n",
    "\n",
    "    \n",
    "class Environment:\n",
    "    def __init__(self, n, m):\n",
    "        self.rowNum = n\n",
    "        self.colNum = m\n",
    "        \n",
    "        #reward for R: S -> relaNumber\n",
    "        self.reward = np.array([[0, 0.45, 1, 0.9]\n",
    "                               ,[0.23, 1.25, 0, 0]\n",
    "                               ,[0, 0.45, 0.75, 0]\n",
    "                               ,[0.85, 1.5, 2.5, 0.85]])\n",
    "        \n",
    "        #Initial Policy for policy iteration  \n",
    "        self.policy =  np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "        \n",
    "        #Probabilty P(action) : A x A-> realNumber\n",
    "                                    #left, right, up, down\n",
    "        self.probability = np.array([[0.8, 0, 0.1, 0.1]    #left\n",
    "                                    ,[0, 0.8, 0.1, 0.1]   #right\n",
    "                                    ,[0.1, 0.1 , 0.8, 0]  #up\n",
    "                                    ,[0.1, 0.1, 0, 0.8]])  #down\n",
    "        \n",
    "        #Initial Value Function\n",
    "        self.value = np.zeros((self.rowNum, self.colNum))\n",
    "        \n",
    "        #Possible Actions\n",
    "        self.action = {\"left\": (0, -1), \"right\": (0, 1), \"up\": (-1, 0), \"down\": (1, 0)}\n",
    "        \n",
    "    #fucntion which takes a state and checks if state if valid i.e state is not outside of grid\n",
    "    def isValidState(self, state):\n",
    "        return (state.row >=0 and state.row < self.rowNum and state.col < self.colNum and state.col >=0)\n",
    "        \n",
    "        \n",
    "    #function which takes current state and action AND returns nextState\n",
    "    def getNextState(self, state, action):\n",
    "        nextState = copy.copy(state)\n",
    "        nextState.row+=self.action[action][0]\n",
    "        nextState.col+=self.action[action][1]\n",
    "        \n",
    "        if(self.isValidState(nextState)):\n",
    "            return nextState\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "class agent:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexToAction = {0:\"left\", 1:\"right\", 2:\"up\", 3:\"down\"}\n",
    "actionToIndex = {\"left\":0, \"right\":1, \"up\":2, \"down\":3}\n",
    "\n",
    "env = Environment(4, 4)\n",
    "\n",
    "def normInfinity(currValue, optimalValue):\n",
    "    maxDiff = 0\n",
    "    for i in range(env.rowNum):\n",
    "        for j in range(env.colNum):\n",
    "            maxDiff = max(maxDiff, abs(currValue[i, j] - optimalValue[i, j]))\n",
    "    return maxDiff\n",
    "            \n",
    "def valueIteration(gamma):\n",
    "    print(\"_________________Value Iteration_________________\")\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -100\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "    #                 print(currState.row, currState.col, currReward)\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "    #     print(delta)\n",
    "        if(delta < theta):\n",
    "            print(\"Values converged....\")\n",
    "            break\n",
    "            \n",
    "    #get Optimal Policy\n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            maxReward = -10\n",
    "            optimalAction = \"left\"\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                        + gamma* env.value[nextState.row, nextState.col]))\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            env.policy[currState.row, currState.col] = optimalAction\n",
    "                \n",
    "\n",
    "    print(\"Reward\\n\", env.reward)\n",
    "    print(\"Optimal Values\\n\",env.value)\n",
    "    print(\"Optimal Policy\\n\", env.policy)\n",
    "    \n",
    "    #Plot ||V_t - V_inf||_inf\n",
    "    optimalValue = env.value.copy()\n",
    "    env.value = np.zeros((env.rowNum, env.colNum))\n",
    "    iter1 = 0\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -1\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "        if(delta < theta):\n",
    "            break\n",
    "        diff.append(normInfinity(env.value, optimalValue))\n",
    "    print(\"Plot of infinity norm of V_t - V*\")\n",
    "    plt.plot(diff, 'r-', label = \"V-V*\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Infinite norm\")\n",
    "    plt.title(\"|| V_t - V* ||_inf  vs  iteration\\n\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"_______________Value Iteration Completed_________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPolicy(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    \n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                prevVal = value[row, col]\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])) \n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "                delta = max(delta, abs(value[row, col] - prevVal))\n",
    "        if(delta < theta):\n",
    "            break\n",
    "    \n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            optimalAction = \"left\"\n",
    "            maxReward = -100\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=p*(env.reward[currState.row, currState.col] + \n",
    "                                       gamma*value[nextState.row, nextState.col])\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            \n",
    "            policy[row, col] = optimalAction\n",
    "            \n",
    "    return policy\n",
    "\n",
    "def getValue(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])) \n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyIteration(gamma):\n",
    "    print(\"\\n___________________Policy Iteration_____________________\")\n",
    "    env = Environment(4, 4)\n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for iter1 in (range(1000)):\n",
    "\n",
    "        prevPolicy = policy.copy()\n",
    "        policy = getPolicy(env, policy, gamma)\n",
    "        if(np.array_equal(policy, prevPolicy)):\n",
    "            print(\"Found optimal Policy\")\n",
    "            break\n",
    "        \n",
    "    print(\"Optimal Policy\\n\", policy)\n",
    "    print(\"Optimal Value\\n\", getValue(env, policy, gamma))\n",
    "    print(\"_________________Policy Iteration Completed________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Call Functions here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________Value Iteration_________________\n",
      "Values converged....\n",
      "Reward\n",
      " [[ 0.    0.45  1.    0.9 ]\n",
      " [ 0.23  1.25  0.    0.  ]\n",
      " [ 0.    0.45  0.75  0.  ]\n",
      " [ 0.85  1.5   2.5   0.85]]\n",
      "Optimal Values\n",
      " [[ 46.03395074  52.34419562  52.49800915  46.92001947]\n",
      " [ 52.08771757  53.88571005  53.28842852  51.5556399 ]\n",
      " [ 52.12578348  53.9757024   54.7997919   52.85915652]\n",
      " [ 48.08416014  53.97680125  55.59643034  49.43279872]]\n",
      "Optimal Policy\n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'left']\n",
      " ['right' 'right' 'down' 'left']\n",
      " ['right' 'up' 'up' 'left']]\n",
      "Plot of infinity norm of V_t - V*\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAElCAYAAADp4+XfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________Value Iteration Completed_________________\n",
      "\n",
      "___________________Policy Iteration_____________________\n",
      "Found optimal Policy\n",
      "Optimal Policy\n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'left']\n",
      " ['right' 'right' 'down' 'left']\n",
      " ['right' 'up' 'up' 'left']]\n",
      "Optimal Value\n",
      " [[ 46.20959513  52.53370762  52.68223783  47.08249321]\n",
      " [ 52.27722813  54.07182142  53.46919662  51.73080361]\n",
      " [ 52.30999811  54.15646912  54.97517776  53.02927681]\n",
      " [ 48.24650986  54.15195159  55.76654931  49.58284379]]\n",
      "_________________Policy Iteration Completed________________\n"
     ]
    }
   ],
   "source": [
    "valueIteration(gamma = 0.98)\n",
    "policyIteration(gamma = 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
