{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 08\n",
    "#### Problem: Grid World\n",
    "##### Members:\n",
    "        1. Amit Vikram Singh(111601001)\n",
    "        2. Kuldeep Singh Bhandari(111601009)\n",
    "Logic :\n",
    "\n",
    "> 1. **Value Iteration** : In value iteration, first we find optimal value for the given grid world and then find the optimal policy. \n",
    "2. **Policy Iteration** : In policy iteration, we have an initial pseudo policy and using that, we will find value corresponding to that policy and then we will update the policy in greedy fashion. We keep on doing that until current policy becomes equal to the previous policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "class State:\n",
    "    def __init__(self, i, j):\n",
    "        self.row = i\n",
    "        self.col = j\n",
    "\n",
    "    \n",
    "class Environment:\n",
    "    def __init__(self, n, m):\n",
    "        self.rowNum = n\n",
    "        self.colNum = m\n",
    "        \n",
    "        #reward for R: S -> relaNumber\n",
    "        self.reward = np.array([[0, 0.45, 1, 0.9]\n",
    "                               ,[0.23, 1.25, 0, 0]\n",
    "                               ,[0, 0.45, 0.75, 0]\n",
    "                               ,[0.85, 1.5, 2.5, 0.85]])\n",
    "        \n",
    "        #Initial Policy for policy iteration  \n",
    "        self.policy =  np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                                ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "        \n",
    "        #Probabilty P(action) : A x A-> realNumber\n",
    "                                    #left, right, up, down\n",
    "        self.probability = np.array([[0.8, 0, 0.1, 0.1]    #left\n",
    "                                    ,[0, 0.8, 0.1, 0.1]   #right\n",
    "                                    ,[0.1, 0.1 , 0.8, 0]  #up\n",
    "                                    ,[0.1, 0.1, 0, 0.8]])  #down\n",
    "        \n",
    "        #Initial Value Function\n",
    "        self.value = np.zeros((self.rowNum, self.colNum))\n",
    "        \n",
    "        #Possible Actions\n",
    "        self.action = {\"left\": (0, -1), \"right\": (0, 1), \"up\": (-1, 0), \"down\": (1, 0)}\n",
    "        \n",
    "    #fucntion which takes a state and checks if state if valid i.e state is not outside of grid\n",
    "    def isValidState(self, state):\n",
    "        return (state.row >=0 and state.row < self.rowNum and state.col < self.colNum and state.col >=0)\n",
    "        \n",
    "        \n",
    "    #function which takes current state and action AND returns nextState\n",
    "    def getNextState(self, state, action):\n",
    "        nextState = copy.copy(state)\n",
    "        nextState.row+=self.action[action][0]\n",
    "        nextState.col+=self.action[action][1]\n",
    "        \n",
    "        if(self.isValidState(nextState)):\n",
    "            return nextState\n",
    "        \n",
    "        return None\n",
    "        \n",
    "        \n",
    "class agent:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexToAction = {0:\"left\", 1:\"right\", 2:\"up\", 3:\"down\"}\n",
    "actionToIndex = {\"left\":0, \"right\":1, \"up\":2, \"down\":3}\n",
    "\n",
    "env = Environment(4, 4)\n",
    "\n",
    "def normInfinity(currValue, optimalValue):\n",
    "    maxDiff = 0\n",
    "    for i in range(env.rowNum):\n",
    "        for j in range(env.colNum):\n",
    "            maxDiff = max(maxDiff, abs(currValue[i, j] - optimalValue[i, j]))\n",
    "    return maxDiff\n",
    "            \n",
    "def valueIteration(gamma):\n",
    "    print(\"_________________Value Iteration_________________\")\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -100\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)+p*gamma* env.value[currState.row, currState.col]\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "    #                 print(currState.row, currState.col, currReward)\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "    #     print(delta)\n",
    "        if(delta < theta):\n",
    "            print(\"Values converged....\")\n",
    "            break\n",
    "            \n",
    "    #get Optimal Policy\n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            maxReward = -10\n",
    "            optimalAction = \"left\"\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)+p*gamma* env.value[currState.row, currState.col]\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                        + gamma* env.value[nextState.row, nextState.col]))\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            env.policy[currState.row, currState.col] = optimalAction\n",
    "                \n",
    "\n",
    "    print(\"Reward\\n\", env.reward)\n",
    "    print(\"Optimal Values\\n\",env.value)\n",
    "    print(\"Optimal Policy\\n\", env.policy)\n",
    "    \n",
    "    #Plot ||V_t - V_inf||_inf\n",
    "    optimalValue = env.value.copy()\n",
    "    env.value = np.zeros((env.rowNum, env.colNum))\n",
    "    iter1 = 0\n",
    "    diff = []\n",
    "    while(True):\n",
    "        if(iter1>1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                value = -1\n",
    "                prevValue = env.value[row, col]\n",
    "                for action in env.action:\n",
    "                    currReward = 0\n",
    "                    for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                        succAction = indexToAction[i]\n",
    "                        nextState = env.getNextState(currState, succAction)\n",
    "                        if(nextState is None):\n",
    "                            currReward+=(p*-1)+p*gamma* env.value[currState.row, currState.col]\n",
    "                        else:\n",
    "                            currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                            + gamma* env.value[nextState.row, nextState.col]))\n",
    "\n",
    "                    value = max(value, currReward)\n",
    "\n",
    "                env.value[row, col] = value\n",
    "                delta = max(delta, abs(prevValue - env.value[row, col]))\n",
    "        if(delta < theta):\n",
    "            break\n",
    "        diff.append(normInfinity(env.value, optimalValue))\n",
    "    print(\"Plot of infinity norm of V_t - V*\")\n",
    "    plt.plot(diff, 'r-', label = \"V-V*\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Infinite norm\")\n",
    "    plt.title(\"|| V_t - V* ||_inf  vs  iteration\\n\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"_______________Value Iteration Completed_________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getPolicy(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    \n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        delta = 0\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                prevVal = value[row, col]\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)+p*gamma* value[currState.row, currState.col]\n",
    "                    else:\n",
    "                        currReward+=(p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])) \n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "                delta = max(delta, abs(value[row, col] - prevVal))\n",
    "        if(delta < theta):\n",
    "            break\n",
    "    \n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for row in range(env.rowNum):\n",
    "        for col in range(env.colNum):\n",
    "            currState = State(row, col)\n",
    "            optimalAction = \"left\"\n",
    "            maxReward = -100\n",
    "            for action in env.action:\n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)+p*gamma* value[currState.row, currState.col]\n",
    "                    else:\n",
    "                        currReward+=p*(env.reward[currState.row, currState.col] + \n",
    "                                       gamma*value[nextState.row, nextState.col])\n",
    "                if(currReward > maxReward):\n",
    "                    maxReward = currReward\n",
    "                    optimalAction = action\n",
    "            \n",
    "            policy[row, col] = optimalAction\n",
    "            \n",
    "    return policy\n",
    "\n",
    "def getValue(env, policy, gamma):\n",
    "    value = np.zeros((env.rowNum, env.colNum))\n",
    "    iter1 = 0\n",
    "    theta = 0.01\n",
    "    while(True):\n",
    "        if(iter1 > 1000):\n",
    "            break\n",
    "        iter1+=1\n",
    "        for row in range(env.rowNum):\n",
    "            for col in range(env.colNum):\n",
    "                currState = State(row, col)\n",
    "                action = policy[row, col]\n",
    "                \n",
    "                currReward = 0\n",
    "                for i, p in enumerate(env.probability[actionToIndex[action]]):\n",
    "                    succAction = indexToAction[i]\n",
    "                    nextState = env.getNextState(currState, succAction)\n",
    "                    if(nextState is None):\n",
    "                        currReward+=(p*-1)+ p*gamma* value[currState.row, currState.col]\n",
    "                    else:\n",
    "                        currReward+= p*(env.reward[currState.row, currState.col]\n",
    "                                         + gamma* value[nextState.row, nextState.col])\n",
    "                    \n",
    "                value[row, col] = currReward\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policyIteration(gamma):\n",
    "    print(\"\\n___________________Policy Iteration_____________________\")\n",
    "    env = Environment(4, 4)\n",
    "    policy = np.array([[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]\n",
    "                        ,[\"right\", \"right\", \"right\", \"right\"]])\n",
    "    \n",
    "    for iter1 in (range(1000)):\n",
    "\n",
    "        prevPolicy = policy.copy()\n",
    "        policy = getPolicy(env, policy, gamma)\n",
    "        if(np.array_equal(policy, prevPolicy)):\n",
    "            print(\"Found optimal Policy\")\n",
    "            break\n",
    "        \n",
    "    print(\"Optimal Policy\\n\", policy)\n",
    "    print(\"Optimal Value\\n\", getValue(env, policy, gamma))\n",
    "    print(\"_________________Policy Iteration Completed________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Call Functions here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________Value Iteration_________________\n",
      "Values converged....\n",
      "Reward\n",
      " [[ 0.    0.45  1.    0.9 ]\n",
      " [ 0.23  1.25  0.    0.  ]\n",
      " [ 0.    0.45  0.75  0.  ]\n",
      " [ 0.85  1.5   2.5   0.85]]\n",
      "Optimal Values\n",
      " [[ 72.12506715  73.90485711  74.02050932  73.07299498]\n",
      " [ 73.72214822  75.4347782   74.77654057  73.17912364]\n",
      " [ 74.40108563  76.07149012  76.81170091  74.98386199]\n",
      " [ 76.22940681  77.56459677  78.14558431  76.80565289]]\n",
      "Optimal Policy\n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' 'right' 'left' 'left']]\n",
      "Plot of infinity norm of V_t - V*\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAElCAYAAADk/ZWYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________Value Iteration Completed_________________\n",
      "\n",
      "___________________Policy Iteration_____________________\n",
      "Found optimal Policy\n",
      "Optimal Policy\n",
      " [['right' 'down' 'down' 'left']\n",
      " ['right' 'down' 'down' 'down']\n",
      " ['down' 'down' 'down' 'down']\n",
      " ['right' 'right' 'left' 'left']]\n",
      "Optimal Value\n",
      " [[ 72.38270059  74.15769494  74.26882075  73.31695265]\n",
      " [ 73.97513365  75.68347685  75.02079756  73.41975333]\n",
      " [ 74.65084836  76.31616785  77.05188903  75.22012671]\n",
      " [ 76.47459282  77.80522906  78.38180123  77.03758823]]\n",
      "_________________Policy Iteration Completed________________\n"
     ]
    }
   ],
   "source": [
    "valueIteration(gamma = 0.98)\n",
    "policyIteration(gamma = 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
